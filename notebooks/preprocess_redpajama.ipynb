{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6296e7-486f-413d-8ba0-9d84bb62a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/gscratch/xlab/alisaliu/hack-tokenizers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635dd7e0-ccd1-498b-bc35-ef5ddaee2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from utils import ensure_dir\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd3630-b053-4885-bedd-a3ad771faeae",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce239fbc-cca1-49f9-b500-f6cf7903838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gscratch/scrubbed/alisaliu/redpajama/wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfb541-5b4a-463c-925d-e4bee068b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(data_dir / 'wiki.jsonl', lines=True)\n",
    "\n",
    "languages = []\n",
    "for meta in tqdm(df.meta):\n",
    "    languages.append(meta['language'])\n",
    "\n",
    "df['language'] = languages\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd552a7-6e2a-4175-a274-dbf781078ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split wikipedia into different languages for shift experiments\n",
    "wiki_by_lang_dir = Path('/gscratch/xlab/alisaliu/redpajama/wikipedia')\n",
    "for language, sub_df in df.groupby('language'):\n",
    "    ensure_dir(wiki_by_lang_dir / language)\n",
    "    with open(wiki_by_lang_dir / language / 'wiki.txt', 'w') as fin:\n",
    "        fin.write('\\n\\n'.join(sub_df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a63c4d-d8ee-46a1-82c9-f8a162f7436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split English wikipedia data into smaller documents\n",
    "english_df = df.loc[df['language'] == 'en'].sample(frac=1)\n",
    "sub_dfs = np.array_split(english_df, 30)\n",
    "for i, sub_df in tqdm(enumerate(sub_dfs), total=len(sub_dfs)):\n",
    "    with open(data_dir / f'{i}.txt', 'w') as fo:\n",
    "        fo.write('\\n\\n'.join(sub_dfs[i]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d2a80-cdf6-4c5e-ae4e-0944634bcde5",
   "metadata": {},
   "source": [
    "# ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78ec9e-de5e-4927-aeed-4a50c4f33863",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gscratch/scrubbed/alisaliu/redpajama/arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b69277-722d-4f1c-b802-e38d183ce4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm(os.listdir(data_dir / 'arxiv')):\n",
    "    if not os.path.isdir(data_dir / 'arxiv' / f):\n",
    "        identifier = f.split('_')[1].rsplit('.', 1)[0]\n",
    "        sub_df = pd.read_json(data_dir / 'arxiv' / f, lines=True)\n",
    "        with open(data_dir / f'{identifier}.txt', 'w') as fo:\n",
    "            fo.write('\\n\\n'.join(sub_df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d083c-d8ef-4605-bc54-6e526b98c1d6",
   "metadata": {},
   "source": [
    "# Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96fbdb-7363-4d44-8b6f-bd035674bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gscratch/scrubbed/alisaliu/redpajama/web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982c9ba-26c0-488e-9000-2e256448aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have way too much data, so really we should downsample from the url list\n",
    "with open(data_dir / '../urls.txt') as fin:\n",
    "    urls = fin.readlines()\n",
    "    cc_urls = [url for url in urls if 'common_crawl' in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e270161-eed4-476a-99a9-49dbf995b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps = defaultdict(list)\n",
    "for url in cc_urls:\n",
    "    dump = url.split('common_crawl/')[1].split('/en')[0]\n",
    "    dumps[dump].append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a97ceb-842f-4b5c-bf54-5227928fd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_urls = []\n",
    "for d in dumps:\n",
    "    sampled_urls.extend(random.sample(dumps[d], len(dumps[d]) // 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb1c54-72a1-4130-81ef-8c0c9620dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir / 'cc_urls.txt', 'w') as fo:\n",
    "    fo.writelines(sampled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f98f9-7a4b-4f5a-8d91-d8abcb94482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dump in os.listdir(data_dir / 'common_crawl'):\n",
    "    for f in tqdm(os.listdir(data_dir / 'common_crawl' / dump), desc=dump):\n",
    "        sub_df = pd.read_json(data_dir / 'common_crawl' / dump / f, lines=True)\n",
    "        identifier = f'{dump}-{f.split(\".\")[0]}'\n",
    "        with open(data_dir / f'{identifier}.txt', 'w') as fo:\n",
    "            fo.write('\\n\\n'.join(sub_df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39237820-4f8c-44e5-b1ae-cdba4547f979",
   "metadata": {},
   "source": [
    "# Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca89a22-f014-4b2e-bb15-e110f70431eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gscratch/scrubbed/alisaliu/redpajama/books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fdd47a-8c05-442f-9f47-bf2c7b3980f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/gscratch/xlab/alisaliu/redpajama/book.jsonl', lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0058fc-b7b3-4d4f-ad3b-9d5f93586473",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dfs = np.array_split(df, 50)\n",
    "for i, sub_df in tqdm(enumerate(sub_dfs)):\n",
    "    with open(data_dir / f'{i}.txt', 'w') as fo:\n",
    "        fo.write('\\n\\n'.join(sub_dfs[i]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd1327a-e9aa-4d99-8835-3c9ea1320068",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b3125-2d72-47a6-9639-0a03cd89e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gscratch/scrubbed/alisaliu/redpajama/github')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198196e-10fb-402a-99ad-776585b3cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fin in tqdm(os.listdir(data_dir / 'github')):\n",
    "    if os.path.isfile(data_dir / 'github' / fin):\n",
    "        random_str = fin.split('_')[1].split('.')[0]\n",
    "        sub_df = pd.read_json(data_dir / 'github' / fin, lines=True)\n",
    "        with open(data_dir / f'{random_str}.txt', 'w') as fo:\n",
    "            fo.write('\\n\\n'.join(sub_df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd8e9-f8b3-485f-9571-bac4616a430e",
   "metadata": {},
   "source": [
    "### Group by language to create language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976b7ea-9fe3-46b8-bc5c-27f99276bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_json = json.load(open('preprocessing/languages.json'))\n",
    "extensions_to_language = {}\n",
    "for language, data in languages_json.items():\n",
    "    if 'extensions' in data:\n",
    "        for ext in data['extensions']:\n",
    "            extensions_to_language[ext[1:]] = language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98084f-ecd2-4adc-8a1a-93686807ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrubbed_data_dir = Path('/gscratch/scrubbed/alisaliu/redpajama/github/github')\n",
    "xlab_data_dir = Path('/gscratch/xlab/alisaliu/redpajama/github')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28ced3-57e3-41f6-8828-ffc181e645f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fin in os.listdir(scrubbed_data_dir):\n",
    "    if os.path.isfile(scrubbed_data_dir / fin):\n",
    "        random_str = fin.split('_')[1].split('.')[0]\n",
    "        df = pd.read_json(scrubbed_data_dir / fin, lines=True)\n",
    "        languages = []\n",
    "        for meta in df.meta:\n",
    "            languages.append(extensions_to_language.get(meta['path'].rsplit('.', 1)[-1].rsplit('/', 1)[-1].lower()))\n",
    "        df['language'] = languages\n",
    "        for language, sub_df in tqdm(df.groupby('language'), desc=random_str):\n",
    "            language = ''.join(language.split(' '))\n",
    "            ensure_dir(xlab_data_dir / language)\n",
    "            with open(xlab_data_dir / language / f'{random_str}.txt', 'w') as fin:\n",
    "                fin.write('\\n\\n'.join(sub_df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969ca5e-e08e-4e69-ba32-9df3b88af7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
